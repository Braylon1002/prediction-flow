{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# script to do experiments described in paper: Deep Interest Evolution Network for Click-Through Rate Prediction\n",
    "\n",
    "## how to run\n",
    "\n",
    "1. Please download data.tar.gz, data1.tar.gz and data2.tar.gz from https://github.com/mouna99/dien and decompress them to the same folder with this script.\n",
    "\n",
    "2. Please run prepare_neg.ipynb first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_MAX_LEN = 100 # maximum sequence length\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 18\n",
    "DNN_HIDDEN_SIZE = [200, 80]\n",
    "DNN_DROPOUT = 0.0\n",
    "TEST_RUN = False\n",
    "EPOCH = 2\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from prediction_flow.features import Number, Category, Sequence, Features\n",
    "from prediction_flow.transformers.column import (\n",
    "    StandardScaler, CategoryEncoder, SequenceEncoder)\n",
    "\n",
    "from prediction_flow.pytorch.data import Dataset\n",
    "from prediction_flow.pytorch import WideDeep, DeepFM, DNN, DIN, DIEN, AttentionGroup\n",
    "\n",
    "from prediction_flow.pytorch.functions import fit, predict, create_dataloader_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    \"./local_train.csv\", sep='\\t')\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    \"./local_test.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_RUN:\n",
    "    train_df = train_df.sample(1000)\n",
    "    valid_df = valid_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_eda(df):\n",
    "    print(df.shape)\n",
    "    print(df.uid.nunique())\n",
    "    print(df.mid.nunique())\n",
    "    print(df.groupby('label', as_index=False).uid.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_eda(train_df)\n",
    "scale_eda(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.values[0][4].split('\\x02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This data set is well balanced. Each user has two samples, pos sample and neg sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cats = Counter(train_df.cat.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cats_in_hist = Counter(\n",
    "    itertools.chain(*train_df.hist_cats.apply(lambda x: x.split(\"\u0002\")).values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_cats), len(unique_cats_in_hist),\n",
    "      len(np.intersect1d(list(unique_cats.keys()), list(unique_cats_in_hist.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All categorys also appear in history categorys.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mids = Counter(train_df.mid.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mids_in_hist = Counter(\n",
    "    itertools.chain(*train_df.hist_mids.apply(lambda x: x.split(\"\u0002\")).values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_mids), len(unique_mids_in_hist),\n",
    "      len(np.intersect1d(list(unique_mids.keys()), list(unique_mids_in_hist.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most mids appears in history mids.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {}% mid overlap between train and valid\".format(\n",
    "    100 * len(np.intersect1d(train_df.mid.unique(), valid_df.mid.unique())) / len(valid_df.mid.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {}% mid overlap between train and valid\".format(\n",
    "    100 * len(np.intersect1d(train_df.cat.unique(), valid_df.cat.unique())) / len(valid_df.cat.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_enc = SequenceEncoder(sep=\"\\x02\", min_cnt=1, max_len=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_enc.fit(train_df.hist_cats.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_word2idx, cat_idx2word = cat_enc.word2idx, cat_enc.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cat_word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_enc = SequenceEncoder(sep=\"\\x02\", min_cnt=1, max_len=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_enc.fit(np.vstack([train_df.mid.values, train_df.hist_mids.values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_word2idx, mid_idx2word = mid_enc.word2idx, mid_enc.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mid_word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features = []\n",
    "\n",
    "category_features = [\n",
    "    Category('mid',\n",
    "             CategoryEncoder(min_cnt=1, word2idx=mid_word2idx, idx2word=mid_idx2word),\n",
    "             embedding_name='mid'),\n",
    "    Category('cat',\n",
    "             CategoryEncoder(min_cnt=1, word2idx=cat_word2idx, idx2word=cat_idx2word),\n",
    "             embedding_name='cat'),\n",
    "]\n",
    "\n",
    "sequence_features = [\n",
    "    Sequence('hist_mids',\n",
    "             SequenceEncoder(sep=\"\\x02\", min_cnt=1, max_len=SEQ_MAX_LEN,\n",
    "                             word2idx=mid_word2idx, idx2word=mid_idx2word),\n",
    "             embedding_name='mid'),\n",
    "    Sequence('hist_cats',\n",
    "             SequenceEncoder(sep=\"\\x02\", min_cnt=1, max_len=SEQ_MAX_LEN,\n",
    "                             word2idx=cat_word2idx, idx2word=cat_idx2word),\n",
    "             embedding_name='cat')\n",
    "]\n",
    "\n",
    "features, train_loader, valid_loader = create_dataloader_fn(\n",
    "    number_features, category_features, sequence_features, BATCH_SIZE, train_df, 'label', valid_df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, df, dataloader):\n",
    "    preds = predict(model, dataloader)\n",
    "    return roc_auc_score(df['label'], preds.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din_attention_groups = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'mid', 'pos_hist': 'hist_mids'},\n",
    "               {'ad': 'cat', 'pos_hist': 'hist_cats'}],\n",
    "        hidden_layers=[80, 40], att_dropout=0.0)]\n",
    "\n",
    "gru_attention_groups = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'mid', 'pos_hist': 'hist_mids'},\n",
    "               {'ad': 'cat', 'pos_hist': 'hist_cats'}],\n",
    "        hidden_layers=[80, 40], att_dropout=0.0, gru_type='GRU')]\n",
    "\n",
    "aigru_attention_groups = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'mid', 'pos_hist': 'hist_mids'},\n",
    "               {'ad': 'cat', 'pos_hist': 'hist_cats'}],\n",
    "        hidden_layers=[80, 40], att_dropout=0.0, gru_type='AIGRU')]\n",
    "\n",
    "agru_attention_groups = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'mid', 'pos_hist': 'hist_mids'},\n",
    "               {'ad': 'cat', 'pos_hist': 'hist_cats'}],\n",
    "        hidden_layers=[80, 40], att_dropout=0.0, gru_type='AGRU')]\n",
    "\n",
    "augru_attention_groups = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'mid', 'pos_hist': 'hist_mids'},\n",
    "               {'ad': 'cat', 'pos_hist': 'hist_cats'}],\n",
    "        hidden_layers=[80, 40], att_dropout=0.0, gru_type='AUGRU')]\n",
    "\n",
    "models = [\n",
    "    ('DNN', DNN(features, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "        final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('WideDeep', WideDeep(features,\n",
    "             wide_features=['mid', 'hist_mids', 'cat', 'hist_cats'],\n",
    "             deep_features=['mid', 'hist_mids', 'cat', 'hist_cats'],\n",
    "             cross_features=[('mid', 'hist_mids'), ('cat', 'hist_cats')],\n",
    "             num_classes=2, embedding_size=EMBEDDING_DIM, hidden_layers=DNN_HIDDEN_SIZE,\n",
    "             final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('DeepFM', DeepFM(features, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE, \n",
    "           final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('DIN', DIN(features, din_attention_groups, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "        final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('DIEN_gru', DIEN(features, gru_attention_groups, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "         final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('DIEN_aigru', DIEN(features, aigru_attention_groups, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "         final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('DIEN_agru', DIEN(features, agru_attention_groups, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "         final_activation='sigmoid', dropout=DNN_DROPOUT)),\n",
    "    \n",
    "    ('DIEN_augru', DIEN(features, augru_attention_groups, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "         final_activation='sigmoid', dropout=DNN_DROPOUT))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(models):\n",
    "    scores = OrderedDict()\n",
    "    model_loss_curves = OrderedDict()\n",
    "    for model_name, model in models:\n",
    "        print(model_name)\n",
    "        loss_func = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "        losses = fit(EPOCH, model, loss_func, optimizer,\n",
    "            train_loader, valid_loader, scheduler, notebook=True, auxiliary_loss_rate=1)\n",
    "    \n",
    "        scores[model_name] = evaluation(model, valid_df, valid_loader)\n",
    "        model_loss_curves[model_name] = losses\n",
    "    return scores, model_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1, model_loss_curves1 = run(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_loss_curves1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features = []\n",
    "\n",
    "category_features = [\n",
    "    Category('mid',\n",
    "             CategoryEncoder(min_cnt=1, word2idx=mid_word2idx, idx2word=mid_idx2word),\n",
    "             embedding_name='mid'),\n",
    "    Category('cat',\n",
    "             CategoryEncoder(min_cnt=1, word2idx=cat_word2idx, idx2word=cat_idx2word),\n",
    "             embedding_name='cat'),\n",
    "]\n",
    "\n",
    "sequence_features = [\n",
    "    Sequence('hist_mids',\n",
    "             SequenceEncoder(sep=\"\u0002\", min_cnt=1, max_len=SEQ_MAX_LEN,\n",
    "                             word2idx=mid_word2idx, idx2word=mid_idx2word),\n",
    "             embedding_name='mid'),\n",
    "    Sequence('hist_cats',\n",
    "             SequenceEncoder(sep=\"\u0002\", min_cnt=1, max_len=SEQ_MAX_LEN,\n",
    "                             word2idx=cat_word2idx, idx2word=cat_idx2word),\n",
    "             embedding_name='cat'),\n",
    "    Sequence('neg_hist_mids',\n",
    "             SequenceEncoder(sep=\"\u0002\", min_cnt=1, max_len=SEQ_MAX_LEN,\n",
    "                             word2idx=mid_word2idx, idx2word=mid_idx2word),\n",
    "             embedding_name='mid'),\n",
    "    Sequence('neg_hist_cats',\n",
    "             SequenceEncoder(sep=\"\u0002\", min_cnt=1, max_len=SEQ_MAX_LEN,\n",
    "                             word2idx=cat_word2idx, idx2word=cat_idx2word),\n",
    "             embedding_name='cat')\n",
    "]\n",
    "\n",
    "features, train_loader, valid_loader = create_dataloader_fn(\n",
    "    number_features, category_features, sequence_features, BATCH_SIZE, train_df, 'label', valid_df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augru_attention_groups_with_neg = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'mid', 'pos_hist': 'hist_mids', 'neg_hist': 'neg_hist_mids'},\n",
    "               {'ad': 'cat', 'pos_hist': 'hist_cats', 'neg_hist': 'neg_hist_cats'}],\n",
    "        hidden_layers=[80, 40], att_dropout=0.0, gru_type='AUGRU')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('DIEN', DIEN(features, augru_attention_groups_with_neg, 2, EMBEDDING_DIM, DNN_HIDDEN_SIZE,\n",
    "         final_activation='sigmoid', dropout=DNN_DROPOUT, use_negsampling=True))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores2, model_loss_curves2 = run(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_loss_curves2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['DIEN'] = scores2['DIEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%f' % height,\n",
    "                ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "rect = ax.bar(list(scores.keys()), list(scores.values()))\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_ylim(top=0.9)\n",
    "autolabel(rect)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
